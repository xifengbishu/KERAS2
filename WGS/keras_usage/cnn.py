#coding:utf-8

'''
    GPU run command:
        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cnn.py
    CPU run command:
        python cnn.py
'''
#å¯æ˜¼å…¥å„ç§ç”¨åˆ°çš„æ¨¡å—ç»„ä»¶
from __future__ import absolute_import
from __future__ import print_function
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.advanced_activations import PReLU
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD, Adadelta, Adagrad, RMSprop
from keras.utils import np_utils, generic_utils
from six.moves import range
from data import load_data
import random


batch_size = 100
nb_epoch = 50


#åŠ è½½æ•°æ®
data, label = load_data()
#æ‰“ä¹±æ•°æ®
index = [i for i in range(len(data))]
random.shuffle(index)
data = data[index]
label = label[index]
print(data.shape[0], ' samples')
print(data.shape, ' data.shape')
#print ( 'data',data[1] )
#labelä¸º0~9å…±10ä¸ªç±»åˆ«ï¼Œkerasè¦æ±‚æ ¼å¼ä¸ºbinary class matrices,è½¬åŒ–ä¸€ä¸‹ï¼Œç›´æ¥è°ƒç”¨kerasæä¾›çš„è¿™ä¸ªå‡½æ•°
#label = np_utils.to_categorical(label, 10)
#print ( 'label',label[1] )

###############
#å¼€å§‹å»ºç«‹CNNæ¨¡å‹
###############

#ç”Ÿæˆä¸€ä¸ªmodel
model = Sequential()

#ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œ4ä¸ªå·ç§¯æ ¸ï¼Œæ¯ä¸ªå·ç§¯æ ¸å¤§å°5*5ã€‚1è¡¨ç¤ºè¾“å…¥çš„å›¾ç‰‡çš„é€šé“,ç°åº¦å›¾ä¸º1é€šé“ã€‚
#border_modeå¯ä»¥æ˜¯validæˆ–è€…fullï¼Œå…·ä½“çœ‹è¿™é‡Œè¯´æ˜ï¼šhttp://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv.conv2d
#æ¿€æ´»å‡½æ•°ç”¨tanh
#ä½ è¿˜å¯ä»¥åœ¨model.add(Activation('tanh'))ååŠ ä¸Šdropoutçš„æŠ€å·§: model.add(Dropout(0.5))
model.add(Convolution2D(4, 5, 5, border_mode='valid',input_shape=data.shape[-3:])) 
model.add(Activation('relu'))


#ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼Œ8ä¸ªå·ç§¯æ ¸ï¼Œæ¯ä¸ªå·ç§¯æ ¸å¤§å°3*3ã€‚4è¡¨ç¤ºè¾“å…¥çš„ç‰¹å¾å›¾ä¸ªæ•°ï¼Œç­‰äºä¸Šä¸€å±‚çš„å·ç§¯æ ¸ä¸ªæ•°
#æ¿€æ´»å‡½æ•°ç”¨tanh
#é‡‡ç”¨maxpoolingï¼Œpoolsizeä¸º(2,2)
model.add(Convolution2D(8, 3, 3, border_mode='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

#ç¬¬ä¸‰ä¸ªå·ç§¯å±‚ï¼Œ16ä¸ªå·ç§¯æ ¸ï¼Œæ¯ä¸ªå·ç§¯æ ¸å¤§å°3*3
#æ¿€æ´»å‡½æ•°ç”¨tanh
#é‡‡ç”¨maxpoolingï¼Œpoolsizeä¸º(2,2)
model.add(Convolution2D(16, 3, 3, border_mode='valid')) 
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

#å…¨è¿æ¥å±‚ï¼Œå…ˆå°†å‰ä¸€å±‚è¾“å‡ºçš„äºŒç»´ç‰¹å¾å›¾flattenä¸ºä¸€ç»´çš„ã€‚
#Denseå°±æ˜¯éšè—å±‚ã€‚16å°±æ˜¯ä¸Šä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ä¸ªæ•°ã€‚4æ˜¯æ ¹æ®æ¯ä¸ªå·ç§¯å±‚è®¡ç®—å‡ºæ¥çš„ï¼š(28-5+1)å¾—åˆ°24,(24-3+1)/2å¾—åˆ°11ï¼Œ(11-3+1)/2å¾—åˆ°4
#å…¨è¿æ¥æœ‰128ä¸ªç¥ç»å…ƒèŠ‚ç‚¹,åˆå§‹åŒ–æ–¹å¼ä¸ºnormal
model.add(Flatten())
model.add(Dense(1, init='normal'))
model.add(Activation('relu'))


#Softmaxåˆ†ç±»ï¼Œè¾“å‡ºæ˜¯10ç±»åˆ«
#model.add(Dense(1, init='normal'))
#model.add(Activation('tanh'))



#############
#å¼€å§‹è®­ç»ƒæ¨¡å‹
##############
#ä½¿ç”¨SGD + momentum
#model.compileé‡Œçš„å‚æ•°losså°±æ˜¯æŸå¤±å‡½æ•°(ç›®æ ‡å‡½æ•°)
sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)
rmspro = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)
model.compile(loss='mean_squared_error', optimizer=rmspro)


#è°ƒç”¨fitæ–¹æ³•ï¼Œå°±æ˜¯ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹. è®­ç»ƒçš„epochæ•°è®¾ä¸º10ï¼Œbatch_sizeä¸º100ï¼
#æ•°æ®ç»è¿‡éšæœºæ‰“ä¹±shuffle=Trueã€‚verbose=1ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­è¾“å‡ºçš„ä¿¡æ¯ï¼Œ0ã€1ã€2ä¸‰ç§æ–¹å¼éƒ½å¯ä»¥ï¼Œæ— å…³ç´§è¦ã€‚show_accuracy=Trueï¼Œè®­ç»ƒæ—¶æ¯ä¸€ä¸ªepochéƒ½è¾“å‡ºaccuracyã€‚
#validation_split=0.2ï¼Œå°†20%çš„æ•°æ®ä½œä¸ºéªŒè¯é›†ã€‚
model.fit(data, label, batch_size=batch_size, nb_epoch=nb_epoch,shuffle=True,verbose=1,validation_split=0.2)
print('Predicting')
predicted_output = model.predict(data[1:5], batch_size=batch_size)
print ( 'label',label[1:5] )
print('predicted_lable',predicted_output)

"""
#ä½¿ç”¨data augmentationçš„æ–¹æ³•
#ä¸€äº›å‚æ•°å’Œè°ƒç”¨çš„æ–¹æ³•ï¼Œè¯·çœ‹æ–‡æ¡£
datagen = ImageDataGenerator(
        featurewise_center=True, # set input mean to 0 over the dataset
        samplewise_center=False, # set each sample mean to 0
        featurewise_std_normalization=True, # divide inputs by std of the dataset
        samplewise_std_normalization=False, # divide each input by its std
        zca_whitening=False, # apply ZCA whitening
        rotation_range=20, # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.2, # randomly shift images vertically (fraction of total height)
        horizontal_flip=True, # randomly flip images
        vertical_flip=False) # randomly flip images

# compute quantities required for featurewise normalization 
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(data)

for e in range(nb_epoch):
    print('-'*40)
    print('Epoch', e)
    print('-'*40)
    print("Training...")
    # batch train with realtime data augmentation
    progbar = generic_utils.Progbar(data.shape[0])
    for X_batch, Y_batch in datagen.flow(data, label):
        loss,accuracy = model.train(X_batch, Y_batch,accuracy=True)
        progbar.add(X_batch.shape[0], values=[("train loss", loss),("accuracy:", accuracy)] )

"""

